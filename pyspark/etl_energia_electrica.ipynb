{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382fe341-9aed-4c8b-a850-5ff1c29f29e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Descripción general del proyecto**\n",
    "\n",
    "Este proyecto tiene como objetivo desarrollar un **pipeline ETL completo** para el análisis del consumo de energía eléctrica en una cadena de tiendas físicas en distintos países de Latinoamérica, utilizando datos operativos reales simulados a nivel diario.\n",
    "\n",
    "A lo largo del proyecto se trabaja con un dataset de alta granularidad que contiene información sobre consumo energético, demanda eléctrica, características físicas de las tiendas y su estado operativo. El enfoque principal es **limpiar, transformar y enriquecer los datos**, garantizando su calidad antes de realizar cualquier análisis agregado.\n",
    "\n",
    "El proyecto se implementa utilizando Pandas, PySpark y SQL, permitiendo comparar enfoques y reforzar el entendimiento de los principios fundamentales de ETL, tales como:\n",
    "\n",
    "* orden correcto del flujo de transformación,\n",
    "* manejo de valores nulos con criterio de negocio,\n",
    "* creación de métricas derivadas relevantes,\n",
    "* uso adecuado de filtros antes y después de agregaciones.\n",
    "\n",
    "Este trabajo está diseñado tanto como ejercicio de aprendizaje profundo como **proyecto demostrable de portafolio** orientado a análisis de datos y data engineering.\n",
    "\n",
    "## Contexto del proyecto\n",
    "\n",
    "Este notebook implementa un pipeline de **ETL distribuido utilizando PySpark**, aplicado a datos diarios de consumo de energía eléctrica en una cadena de tiendas.\n",
    "\n",
    "El objetivo principal es replicar y escalar las transformaciones realizadas en Pandas, utilizando operaciones propias de Spark como:\n",
    "\n",
    "- filtros distribuidos,\n",
    "- imputaciones basadas en agregaciones por grupo,\n",
    "- creación de columnas derivadas,\n",
    "- validación de calidad de datos.\n",
    "\n",
    "El proyecto enfatiza el uso correcto del **orden de las transformaciones**, la separación entre limpieza y análisis, y el manejo de datos faltantes con criterios realistas de negocio.\n",
    "Este enfoque es representativo de escenarios reales en plataformas de big data como Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4646cd22-8a9d-4d6c-b133-6de9f8bc8773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, coalesce\n",
    "\n",
    "\n",
    "# INICIALIZACIÓN DE SPARK\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName('elt_energia_electrica')\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# CARGA DEL DATASET\n",
    "df = spark.read.csv(\n",
    "    '/Volumes/workspace/default/raw_data/consumo_energia_tiendas_diario_2023_2024.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "\n",
    "# ESQUEMA DE LOS DATOS\n",
    "df.show()\n",
    "\n",
    "\n",
    "# INFORMACIÓN DEL DATASET\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# VERIFICAMOS NULOS EN COLUMNAS\n",
    "for columna in df.columns:\n",
    "    print(\n",
    "        f\"Nulos en {columna}: \",\n",
    "        df.filter(col(columna).isNull()).count()\n",
    "    )\n",
    "\n",
    "\n",
    "'''\n",
    "Aunque las tiendas presentan consumo eléctrico en días cerrados debido a cargas base y sistemas auxiliares, estos registros no representan operación comercial normal. Para evitar distorsionar métricas de eficiencia energética, se excluyen de los análisis operativos.\n",
    "\n",
    "Existen 5482 registros nulos en las columnas de area_m2, consumo_kwh y demanda_kw. Ya que es muy importante conservar la información completa \n",
    "de estas columnas, imputaremos los valores utilizando el promedio de los datos, considerando el tipo de tienda y el país donde se encuentran \n",
    "localizadas. \n",
    "'''\n",
    "\n",
    "\n",
    "# FILTRADO DE TIENDAS ACTIVAS\n",
    "df_active = df.filter(col(\"estado_tienda\") == 'Activa')\n",
    "\n",
    "\n",
    "# DETERMINAMOS PROMEDIO DE DATOS\n",
    "promedio = (\n",
    "    df_active\n",
    "        .groupBy(['pais', 'tipo_tienda'])\n",
    "        .agg(\n",
    "            avg(col('area_m2')).alias('promedio_area_m2'),\n",
    "            avg(col('consumo_kwh')).alias('promedio_consumo_kwh'),\n",
    "            avg(col('demanda_kw')).alias('promedio_demanda_kw')\n",
    "        )\n",
    ")\n",
    "\n",
    "promedio.show()\n",
    "\n",
    "\n",
    "# UNIMOS PROMEDIOS AL DATASET ACTIVO\n",
    "df_active = df_active.join(\n",
    "    promedio,\n",
    "    on=['pais', 'tipo_tienda'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "# IMPUTAMOS VALORES EN COLUMNAS OBJETIVO\n",
    "df_active = df_active.withColumn(\n",
    "    \"area_m2\",\n",
    "    coalesce(col('area_m2'), col(\"promedio_area_m2\"))\n",
    ")\n",
    "\n",
    "df_active = df_active.withColumn(\n",
    "    \"consumo_kwh\",\n",
    "    coalesce(col('consumo_kwh'), col(\"promedio_consumo_kwh\"))\n",
    ")\n",
    "\n",
    "df_active = df_active.withColumn(\n",
    "    \"demanda_kw\",\n",
    "    coalesce(col('demanda_kw'), col(\"promedio_demanda_kw\"))\n",
    ")\n",
    "\n",
    "\n",
    "# ELIMINAMOS COLUMNAS AUXILIARES\n",
    "df_active = df_active.drop(\n",
    "    \"promedio_area_m2\",\n",
    "    \"promedio_consumo_kwh\",\n",
    "    \"promedio_demanda_kw\"\n",
    ")\n",
    "\n",
    "\n",
    "# CREACIÓN DE KPIs\n",
    "df_active = df_active.withColumn(\n",
    "    'kpi_kwh/m2',\n",
    "    col(\"consumo_kwh\") / col(\"area_m2\")\n",
    ")\n",
    "\n",
    "df_active = df_active.withColumn(\n",
    "    'kpi_kw/m2',\n",
    "    col(\"demanda_kw\") / col(\"area_m2\")\n",
    ")\n",
    "\n",
    "\n",
    "# VERIFICAMOS NULOS EN COLUMNAS\n",
    "for columna in df_active.columns:\n",
    "    print(\n",
    "        f\"Nulos en {columna}: \",\n",
    "        df_active.filter(col(columna).isNull()).count()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f93ac1f-db91-483f-82e3-4516fc972d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_active.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd05b1a-f497-421d-9dfe-9bd8ccccad5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_energia_electrica",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
